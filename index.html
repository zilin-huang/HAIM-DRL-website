<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Academic Project Page</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Human as AI Mentors: Enhanced Deep Reinforcement Learning with Human Intervention for Safe and Efficient Autonomous Driving</h1>
              <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=RgO7ppoAAAAJ&hl=en" target="_blank">Zilin Huang</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=3T-SILsAAAAJ&hl=en" target="_blank">Zihao Sheng</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=6lAO0WsAAAAJ&view_op=list_works&sortby=pubdate" target="_blank">Chengyuan Ma</a>,</span>
                    <span class="author-block">
                      <a href="https://scholar.google.com/citations?user=DPN2wc4AAAAJ&hl=en" target="_blank">Sikai Chen</a><sup>*</sup>
                      </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Wisconsin-Madison</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/sample.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zilin-huang/HAMP-DRL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Video link -->
              <span class="link-block">
                <a href="https://www.youtube.com/playlist?list=PL-EmC8vF-RSH2j09uxZeyiVV_ePuA3Eud" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-video"></i> <!-- Video icon -->
                </span>
                <span>Video</span>
              </a>
            </span>
              

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- Adjusted YouTube video embed to match the style of the other YouTube video section -->
      <iframe width="100%" height="600" src="https://www.youtube.com/embed/nf7CTmIysDU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>Despite significant progress in autonomous driving, the formulation of a driving policy that ensures the safety and traffic efficiency of autonomous vehicles (AVs) has not yet been fully explored. By integrating the strengths of the transportation and robotics communities, this paper presents an enhanced deep reinforcement learning (DRL) method, which facilitates safe and efficient autonomous driving in mixed traffic platoons. Drawing inspiration from the human learning process, we first introduce an innovative learning paradigm that effectively injects human intelligence into artificial intelligence (AI), which is termed <strong>human-AI mentor paradigm (HAMP)</strong>. A distinctive feature of HAMP is that the human subject serves as a mentor to the AI agent, supervising, intervening, and demonstrating in its learning process. 
            <br>
            <br>
            Building upon this concept, we propose a HAMP-based DRL framework, named <strong>HAMP-DRL</strong>, which ensures the safety of the AV itself while reducing traffic disturbance, thereby optimizing traffic efficiency. In particular, in the HAMP-DRL, we circumvent the complex task of manually crafting a reward function, a common challenge in traditional DRL. Instead, we directly utilize human intervention signals to guide the agent's policy learning. Additionally, we incorporate a technique to minimize intervention, reducing the human mentor's workload. <strong>Comparative results indicate that HAMP-DRL outperforms traditional methods in driving safety, sampling efficiency, traffic disturbance mitigation, and generalizability to unseen traffic environments</strong>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Single Image with Title -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="max-width: 80%; margin: auto; text-align: left;">Overview</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 80%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/paper-flow.jpg" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
        <h2 class="subtitle" style="font-size: medium; text-align: justify;"> <!-- Adjusted font size and alignment -->
          Overview of our proposed human intervention-based DRL framework. (a) The 'X+1+N' mixed traffic platoon scenario is a novel concept uniting the transportation and robotics domains. This scenario is characterized by a combination of HVs and an AV, navigating through a dynamic and uncertain traffic environment. (b) The HAMP is an innovative learning paradigm that infuses human intelligence into the fabric of AI, enhancing the learning capabilities of AI agents. The framework further integrates HAMP into the training loop of RL. Additionally, the framework can be seamlessly embedded into the MetaDrive/CARLA driving environment for testing.
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Single Image with Title -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="max-width: 80%; margin: auto; text-align: left;">Human-AI Mentor Paradigm</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 80%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/Human-AI Mentor Paradigm.jpg" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
        <h2 class="subtitle" style="font-size: medium; text-align: justify;"> <!-- Adjusted font size and alignment -->
          In traditional RL paradigm, as shown in (a), it heavily relies on exploration through trials and errors, which requires significant training time and inevitably puts the agent in dangerous situations. On the other hand, passive human involvement, as depicted in (b), merely provides suggestions about which actions are appropriate or evaluates collected trajectories. While it helps to shorten the training time, it still exposes the agent to potential hazards. When observing the human learning process in daily life, especially in acquiring practical skills, one notices that it does not rely solely on trial and error. Instead, people often employ explicit and implicit interventions to improve learning efficiency and ensure the safety of the learning process. As an illustration, consider the driving school scenario depicted in (c).       
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Single Image with Title -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="max-width: 80%; margin: auto; text-align: left;">Explicit and Implicit Intervention Mechanism</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 80%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/mechanism.jpg" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
        <h2 class="subtitle" style="font-size: medium; text-align: justify;"> <!-- Adjusted font size and alignment -->
          Explicit intervention and implicit intervention mechanism in HAMP. (a) The former involves humans actively taking direct control of the AV agent, guiding it through correct behaviors in hazardous scenarios. (b) The latter involves penalizing the agent for actions that disrupt traffic, indirectly indicating that it should avoid such actions in the future.     
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Single Image with Title -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Adjusted Title with left alignment -->
      <h2 class="title is-3" style="max-width: 80%; margin: auto; text-align: left;">HAMP-DRL Framework</h2>
      <br>
      <div class="image-container has-text-centered" style="max-width: 80%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/HAMP-DRL.jpg" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
        <h2 class="subtitle" style="font-size: medium; text-align: justify;"> <!-- Adjusted font size and alignment -->
          Schematic of the HAMP-DRL framework for driving policy learning. We first delineate the observation space of our HAMP-DRL framework, which is designed to accommodate two distinct types of sensory inputs: (i) The first type pertains to numerical observation spaces that encompass data such as current vehicular states, navigation information, and surrounding information encoded by LiDAR. (ii) The second type addresses visual observation spaces, which include image inputs from mounted cameras. Specifically, we employ a convolutional neural network (CNN) to extract feature vectors, which are then combined with velocity. Then, we introduce a reward-free policy learning method that explicitly conveys human intentions to the learning policy through human intervention. To encourage agent exploration in the permissible state-action space, we maximize the entropy of the agent's action distribution if the agent is not subject to takeover. Simultaneously, we implement measures to minimize takeover and disturbance costs. 
        </h2>
      </div>
    </div>
  </div>
</section>



<!-- Single Image with Title -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <!-- Adjusted Title with left alignment -->
      <h2 class="title is-3" style="max-width: 80%; margin: auto; text-align: left;">Experiment Results</h2>
      <br>
      <div class="image-container has-text-centered" style="max-width: 80%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/experiment results.png" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
        <h2 class="subtitle" style="font-size: medium; text-align: justify;"> <!-- Adjusted font size and alignment -->
          <p>Specifically, we are most interested in four aspects of performance: 
            (a) <strong>safety</strong>, which includes avoiding obstacles during both the training and testing phases, as well as ensuring the AV reaches its destination; 
            (b) <strong>generalization ability</strong>, evaluating the agents on new maps not encountered during training to verify their ability to navigate safely in new environments; 
            (c) <strong>sampling efficiency</strong>, the time required for the method to demonstrate satisfactory performance; and 
            (d) <strong>minimizing traffic flow disturbance</strong>, which aims to enhance traffic efficiency.</p>
        </h2>
      </div>
    </div>
  </div>
</section>



<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3" style="max-width: 80%; margin: auto; text-align: left;">Visualization of Comparison Results</h2>
      <br>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/Cp6XWBsbfRI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3" style="max-width: 80%; margin: auto; text-align: left;">Demonstration Video for Each Model</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <!-- YouTube video embed -->
          <iframe src="https://www.youtube.com/embed/21slajOv8oc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        <div class="item item-video2">
          <!-- YouTube video embed -->
          <iframe src="https://www.youtube.com/embed/RQXO4VICvxQ" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        <div class="item item-video3">
          <!-- YouTube video embed -->
          <iframe src="https://www.youtube.com/embed/zFpEhVv0yRA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        <div class="item item-video4">
          <!-- YouTube video embed -->
          <iframe src="https://www.youtube.com/embed/mmLVAJdfH0M" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        <div class="item item-video5">
          <!-- YouTube video embed -->
          <iframe src="https://www.youtube.com/embed/wKl6Nidm_dk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        <div class="item item-video6">
          <!-- YouTube video embed -->
          <iframe src="https://www.youtube.com/embed/gDMhYYe7JA8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->




<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <!-- <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
